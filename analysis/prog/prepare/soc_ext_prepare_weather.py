# -*- coding: utf-8 -*-
"""
Created on Mon Sep 16 11:25:50 2019

@author: Fabel

Steps:

     1) read in the time series for all of the variables and generate a wide
     version, i.e. a panel on the monitor-day level

     2) use the monitors, which are present at all of the observed years and which
     observe all the weather variables at the same time (see comment below) and
     find with QGIS the nearest associated weather monitor to each stadium.
     use that as input to restrict the wide weather df to the relevant monitors.
     -> exported as weather_prepare to Dropbox


Inputs:
     - map_stadium_nearest_weather_monitor.csv    [intermed_maps]
         - comes as output from QGIS, generated by "Distance to nearest hub" has to be copied from DX
         - maps the stadiums to the nearest weather monitor
         - in an earlier version of the program, it was read_in directly from Dx

     - data_XXX.csv    [source]       contains the values per weather variable on the daily level, only in F, due to the size



Outputs
     - weather_monitors_coordinates_outer_merge.csv [intermed]
          - is used by QGIS
     - weather_prepared.csv [final]




Updates: 26/09/2019:
    a) error in building up the weather df, merge has to be outer, otherwise it drops dates from the df!
    b) NaNs in the column are no filled with preceeding values

    - 02/10/2019: select monitors that are present over all waves, selection now
        on start and end date of crime data time window

"""

# packages
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.style as style
from datetime import datetime
import time
start_time = time.time()


# paths work (SERVER)
z_weather_source =                  'F:/econ/soc_ext/analysis/data/source/weather/cdc_download_2019-08-28_11-40/data/'
z_weather_output_intermed =         'F:/econ/soc_ext/analysis/data/intermediate/weather/'
z_map_input_intermed =              'F:/econ/soc_ext/analysis/data/intermediate/maps/'
z_weather_output_final =            'F:/econ/soc_ext/analysis/data/final/'
z_weather_figures_desc =            'F:/econ/soc_ext/analysis/output/graphs/descriptive/'
z_prefix =                          'soc_ext_'


# paths work (LOCAL)
#z_weather_source =                  'F:/econ/soc_ext/analysis/data/source/weather/cdc_download_2019-08-28_11-40/data/'
#z_weather_output_intermed =         'C:/Users/fabel/Dropbox/soc_ext_Dx/analysis/data/intermediate/weather/'
#z_map_input_intermed =              'C:/Users/fabel/Dropbox/soc_ext_Dx/analysis/data/intermediate/maps/'
#z_weather_output_final =            'C:/Users/fabel/Dropbox/soc_ext_Dx/analysis/data/final/'
#z_weather_figures_desc =            'F:/econ/soc_ext/analysis/output/graphs/descriptive/'
#z_prefix =                          'soc_ext_'


# HOME directories
#z_weather_output_Dx = '/Users/marcfabel/Dropbox/soc_ext_Dx/analysis/data/intermediate/weather/'
#z_map_input_intermed = '/Users/marcfabel/Dropbox/soc_ext_Dx/analysis/data/intermediate/maps/' # comes as output from QGIS, generated by "Distance to nearest hub"
#soccer_output = '/Users/marcfabel/Dropbox/soc_ext_Dx/analysis/data/intermediate/soccer/'


# magic numbers
z_first_year_wave = 2011
z_last_year_wave = 2015
z_start_date = datetime(z_first_year_wave,1,1)      # is the time window from the crime data
z_end_date = datetime(z_last_year_wave,6,30)


###############################################################################
#       1)    Read in and merge the data
###############################################################################

# generate empty lists -> used to construct df that contains # of monitors for each var
lst_var = []
lst_number_monitors = []

j = 1
for var in ['TMK', 'TXK', 'TNK', 'TGK', 'VPM', 'NM', 'PM', 'UPM', 'RS', 'SDK', 'SH', 'FM']: #
     print(var)
     data = pd.read_csv(z_weather_source + 'data_' + var + '.csv', sep=',', encoding = 'UTF-8')
     data.drop(['Produkt_Code','Qualitaet_Niveau', 'Qualitaet_Byte'], axis=1, inplace=True)
     data.rename(columns={'Zeitstempel':'date'}, inplace=True)
     data['year'] = pd.to_numeric(data['date'].astype(str).str.slice(0,4))
     delete_rows = data[ (data['year']< z_first_year_wave) | (data['year']>z_last_year_wave) ].index
     data.drop(delete_rows, inplace=True)
     data.drop('year', axis=1, inplace=True)
     data.rename(columns={'Wert':var}, inplace=True)

     sdo = pd.read_csv(z_weather_source + 'sdo_' + var + '.csv', sep=',', encoding = 'UTF-8')
     sdo.drop(['Metadata_Link', 'Hoehe_ueber_NN'], axis=1, inplace=True)
     sdo.rename(columns={'Geogr_Laenge':'geo_x', 'Geogr_Breite':'geo_y'}, inplace=True)

     lst_var.append(var)
     lst_number_monitors.append(len(sdo))

     # merge data and sdo together
     data = data.merge(sdo, on=['SDO_ID'])

     # put together final data set
     if j == 1:
          weather = data
          j = j + 1
     else:
          weather = weather.merge(data, on=['SDO_ID', 'SDO_Name', 'geo_x', 'geo_y', 'date'], how='outer')

dictionary = {'variable': lst_var, 'number_monitors': lst_number_monitors}
df_number_monitors = pd.DataFrame(dictionary)



###############################################################################
#       2 ) Weather monitors & their distance to stadiums
###############################################################################
# per monitor what is start date and what end date:
monitors = weather.copy()   # [['SDO_ID', 'date', 'SDO_Name', 'geo_x', 'geo_y']]
monitors['min'] = monitors.groupby('SDO_ID')['date'].transform('min')
monitors['max'] = monitors.groupby('SDO_ID')['date'].transform('max')
# count number of observations per weather variable
for var in ['TMK', 'TXK', 'TNK', 'TGK', 'VPM', 'NM', 'PM', 'UPM', 'RS', 'SDK', 'SH', 'FM']: #
    monitors[var] = monitors.groupby('SDO_ID')[var].transform('count')
monitors = monitors.drop_duplicates(subset='SDO_ID')
monitors['date_min'] = pd.to_datetime(monitors['min'], format='%Y%m%d') # , errors='coerce'
monitors['date_max'] = pd.to_datetime(monitors['max'], format='%Y%m%d') # , errors='coerce'

# drop monitors which are not present over the entire period:
delete_rows = monitors[ (monitors['date_min'] > z_start_date) | (monitors['date_max'] < z_end_date) ].index
monitors.drop(delete_rows, inplace=True)

# drop monitors, which do not measure one variable
#monitors[['TMK', 'TXK', 'TNK', 'TGK', 'VPM', 'NM', 'PM', 'UPM', 'RS', 'SDK', 'SH', 'FM']].astype(bool).sum(axis=0)
#TMK    484
#TXK    463
#TNK    463
#TGK    478
#VPM    484
#NM     470
#PM     197     mittlerer Luftdruck in hpa
#UPM    484
#RS     467
#SDK    296     Sonnenscheindauer in Stunden
#SH     457
#FM     210     mittlere Windgeschwindigkeit
for var in ['TMK', 'TXK', 'TNK', 'TGK', 'VPM', 'NM', 'PM', 'UPM', 'RS', 'SDK', 'SH', 'FM']: #
    monitors.drop( monitors[ (monitors[var] == 0 )].index, inplace = True)


# delete monitor if more than 10% of observations must be filled in (except for SH)
for var in ['TMK', 'TXK', 'TNK', 'TGK', 'VPM', 'NM', 'PM', 'UPM', 'RS', 'SDK', 'FM']: # ! SH IS NOT PART OF THE LIST!
    monitors.drop(monitors[ monitors[var] < 0.9 * monitors[var].max()].index, inplace= True)


# read out
monitors[['SDO_ID', 'SDO_Name', 'geo_x', 'geo_y']].to_csv(z_weather_output_intermed + 'weather_monitors_coordinates_outer_merge.csv', sep=';', encoding='UTF-8')   #name and location changed 26/09/2019


# now go to QGIS and look for the nearest stadium-monitor pair and use only subset of monotors for output
# has to create     map_stadium_nearest_weather_monitor.csv'

# open up edited stadium-monitor pair
stadium_monitor = pd.read_csv(z_map_input_intermed + 'map_region_nearest_weather_monitor.csv', sep=';', encoding = 'UTF-8')
stadium_monitor = stadium_monitor[['AGS', 'stadium', 'Ort', 'HubName', 'HubDist']]
stadium_monitor.rename(columns={'HubName':'SDO_ID', 'HubDist':'distance_closest_sdo'}, inplace=True)

# which ones are the active monitors (the closest monitor to the centroid of the region)
active_monitors = stadium_monitor.copy().drop_duplicates(subset='SDO_ID')
active_monitors.drop(['AGS', 'stadium', 'Ort'], inplace=True, axis=1)

# map monitor -> region
map_monitor_region = stadium_monitor[['AGS', 'SDO_ID']].copy()
map_monitor_region =  map_monitor_region.drop_duplicates(subset='AGS')

# merge with weather data (inner)
weather = weather.merge(active_monitors, on=['SDO_ID'], how='inner')





###############################################################################
#       3 ) HANDLING OF MISSING DATA
###############################################################################
weather['date'] = pd.to_datetime(weather['date'], format='%Y%m%d')
weather['month'] = weather.date.apply(lambda x: x.strftime('%m')).astype(int)

# fill missings of SH (snow height) in summer with zeros:
make_zeros = (weather.SH.isnull()) & (weather.month >4) & (weather.month < 10)
weather.loc[make_zeros, 'SH'] = 0

# Which variables have the most missing - see description
for var in ['TMK', 'TXK', 'TNK', 'TGK', 'VPM', 'NM', 'PM', 'UPM', 'RS', 'SDK', 'SH', 'FM']:
    print(weather[var].isnull().value_counts())

 # use forward fill - i.e. if one value of one monitor is missing
 #  then it will use the value of the preceding day
 # copy across monitors is not a problem as the time window is too wide
for var in weather[['TMK', 'TXK', 'TNK', 'TGK', 'VPM', 'NM', 'PM', 'UPM', 'RS', 'SDK', 'SH', 'FM']]:
    weather[var].fillna(method='ffill', inplace=True)





###############################################################################
#       4 ) read out
###############################################################################
temp = weather.copy()

# add information about which monitor belongs to which AGS
weather = weather.merge(map_monitor_region, on=['SDO_ID'])

# sort & order
weather = weather[[
     'date', 'AGS', 'SDO_ID', 'SDO_Name',
     'TMK', 'TXK', 'TNK', 'TGK', 'VPM', 'NM', 'PM', 'UPM', 'RS', 'SDK', 'SH', 'FM',
     'distance_closest_sdo', 'geo_x', 'geo_y']]

weather.sort_values(['AGS','date'], inplace=True)

weather.reset_index(inplace=True, drop=True)

# drop variables
weather.drop(['geo_x', 'geo_y'], axis=1, inplace=True)


# read out
weather.to_csv(z_weather_output_final + 'weather_prepared.csv', sep=';', encoding='UTF-8')

print(len(weather.drop_duplicates(subset='AGS')))

###############################################################################
#       5) Plotting distance of weather monitors to stadiums
###############################################################################



style.available
style.use('seaborn-white')

#style.use('seaborn-paper') # alternative talk and presentation - makes it bigger
#sns.set_context('paper')


# histogram unweighted
ax = sns.distplot(stadium_monitor['distance_closest_sdo'], hist=True, kde=False, norm_hist=True,
             bins=int(50/4), color = 'darkblue',
             hist_kws={'edgecolor':'black'})

plt.xlabel('Distance [km]')
plt.ylabel('Density') # 'Number of monitor-stadium pairs'
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

plt.savefig(z_weather_figures_desc + z_prefix + 'distance_monitors_stadiums.pdf')


###############################################################################
#           END OF FILE
###############################################################################
print("--- %s seconds ---" % (time.time() - start_time))




# Comments:
# - first I use the subset of monitors that have all the variables over all years
#    at a later robustness check, one could refine that approach: i.e. what is per
#    variable the closest monitor, have a weather_variable-specific stadium-monitor
#    match








# OLD:
# 1) Plotting
# other approach
#bins = np.arange(0,50,4)
#hist, edges = np.histogram(stadium_monitor['distance_closest_sdo'], bins)
#freq = hist/float(hist.sum())
#plt.bar(bins[:-1], freq, width=4, align="edge", ec="k" )
#plt.show()
#
#sns.barplot(bins[:-1], freq)
#
#
#
## weighted in how foten it appears in the data
#df = pd.read_csv(soccer_output + 'soccer_prepared.csv', sep=';')
#df = df.merge(stadium_monitor, on='stadium')
#
#sns.distplot(df['distance_closest_sdo'], hist=True, kde=False, norm_hist=True,
#             bins=int(50/4), color = 'darkblue',
#             hist_kws={'edgecolor':'black'})
#
#plt.title('Histogram of Distances from weather monitor to stadiums')
#plt.xlabel('Distance (km)')
#plt.ylabel('Number of monitor-stadium pairs')

